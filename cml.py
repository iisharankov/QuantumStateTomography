import numpy as np

from keras.models import Sequential, load_model
from keras.layers.core import Dense, Activation
from keras.optimizers import SGD
import keras.backend as K



def trainModel(modelName):

    model = Sequential()
    model.add(Dense(12, input_dim=12))
    model.add(Activation('sigmoid'))
    model.add(Dense(10))
    model.add(Activation('sigmoid'))
    model.add(Dense(8))
    model.add(Activation('sigmoid'))

    # def custom_loss(y_true, y_pred):
    #     loss = K.mean(K.square(K.abs(y_pred - y_true)))
    #     return loss


    sgd = SGD(lr=0.5)
    # model.compile(loss=custom_loss, optimizer=sgd)
    model.compile(loss='mean_squared_error', optimizer=sgd)


    model.fit(xQM, yTrue, batch_size=1, epochs=500)
    np.set_printoptions(threshold=np.inf)

    model.save(f'{modelName}.h5')  # creates a HDF5 file 'my_model.h5'

def outputModel(model):
    # print(model.predict(xQM))
    print(model.predict(xQM)[0])
    print(yTrue[0])
    # print(model.predict(xQM)[1])
    # print(yTrue[1])
    # print("\n DIFFERENCE \n")
    # inn = np.abs(yTrue - model.predict(xQM))
    # output = [np.round(i, 5) for i in inn]
    # print(inn[0:4])

def testModel(model):
    x_test = np.array([[0.249765, 0.249633, 0.011529, 0.24994, 0.250384, 0.0, 0.250855, 0.250152, 0.0, 0.24944, 0.249831, 0.988471],
         [0.250157, 0.250168, 0.677278, 0.249822, 0.250253, 0.0, 0.249999, 0.249784, 0.0, 0.250022, 0.249795, 0.322722],
         [0.249502, 0.249906, 0.545948, 0.249786, 0.250204, 0.0, 0.249965, 0.25035, 0.0, 0.250747, 0.24954, 0.454052],
         [0.249203, 0.249516, 0.013751, 0.250092, 0.249521, 0.0, 0.250525, 0.25055, 0.0, 0.25018, 0.250413, 0.986249],
         [0.250334, 0.249709, 0.008709, 0.250278, 0.250521, 0.0, 0.24934, 0.250036, 0.0, 0.250048, 0.249734, 0.991291]])
    y_test = np.array([[0.10726517995466726, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.9942304467120753],
                       [0.8226890064011878, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.56849168749123],
                       [0.739462070159504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.673198222513563],
                       [0.11720331427138621, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.993107941325515],
                       [0.09373753691791456, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.9955969436335984]])

    score = model.evaluate(x_test, y_test, verbose=0)
    print(score)
    score = model.evaluate(xQM, yTrue, verbose=0)
    print(score)

if __name__ == '__main__':
    xQM = np.array([
        [0.249973, 0.250044, 0.49999, 0.250192, 0.24918, 0.0, 0.249474, 0.250612, 0.0, 0.250361, 0.250164, 0.50001],
        [0.25106, 0.249696, 0.008486, 0.250279, 0.249752, 0.0, 0.249642, 0.249955, 0.0, 0.249019, 0.250597, 0.991514],
        [0.250135, 0.250186, 0.887886, 0.250277, 0.249145, 0.0, 0.250103, 0.250382, 0.0, 0.249485, 0.250287, 0.112114],
        [0.250191, 0.250105, 0.369064, 0.249611, 0.249968, 0.0, 0.250457, 0.249517, 0.0, 0.249741, 0.25041, 0.630936],
        [0.250265, 0.250403, 0.979554, 0.2511, 0.249989, 0.0, 0.249357, 0.249848, 0.0, 0.249278, 0.24976, 0.020446],
        [0.250207, 0.249974, 0.968989, 0.250473, 0.250451, 0.0, 0.24943, 0.249841, 0.0, 0.24989, 0.249734, 0.031011],
        [0.250356, 0.24939, 0.875324, 0.250062, 0.250581, 0.0, 0.249814, 0.249579, 0.0, 0.249768, 0.25045, 0.124676],
        [0.250159, 0.249694, 0.994838, 0.249791, 0.249874, 0.0, 0.250158, 0.250044, 0.0, 0.249892, 0.250388, 0.005162],
        [0.249934, 0.250753, 0.919587, 0.250296, 0.250136, 0.0, 0.249881, 0.249234, 0.0, 0.249889, 0.249877, 0.080413],
        [0.249764, 0.250537, 0.045874, 0.250235, 0.250293, 0.0, 0.250197, 0.249535, 0.0, 0.249804, 0.249635, 0.954126],
        [0.249874, 0.24917, 0.011076, 0.249532, 0.250731, 0.0, 0.249806, 0.249752, 0.0, 0.250788, 0.250347, 0.988924],
        [0.249667, 0.249794, 0.087557, 0.249993, 0.249944, 0.0, 0.250272, 0.250444, 0.0, 0.250068, 0.249818, 0.912443],
        [0.250271, 0.250156, 0.171574, 0.249909, 0.250134, 0.0, 0.249827, 0.249751, 0.0, 0.249993, 0.249959, 0.828426],
        [0.249765, 0.25001, 0.169125, 0.250098, 0.24965, 0.0, 0.250696, 0.250365, 0.0, 0.249441, 0.249975, 0.830875],
        [0.250844, 0.250338, 0.938471, 0.249383, 0.250278, 0.0, 0.249542, 0.249875, 0.0, 0.250231, 0.249509, 0.061529],
        [0.249964, 0.249811, 0.108173, 0.249469, 0.249341, 0.0, 0.249342, 0.250121, 0.0, 0.251225, 0.250727, 0.891827],
        [0.249762, 0.249657, 0.004592, 0.250328, 0.250194, 0.0, 0.249499, 0.250243, 0.0, 0.250411, 0.249906, 0.995408],
        [0.249739, 0.249906, 0.67724, 0.250614, 0.249562, 0.0, 0.250087, 0.24951, 0.0, 0.24956, 0.251022, 0.32276],
        [0.249232, 0.249371, 0.030887, 0.250216, 0.249611, 0.0, 0.250643, 0.250976, 0.0, 0.249909, 0.250042, 0.969113],
        [0.250385, 0.250357, 0.000288, 0.249814, 0.250184, 0.0, 0.249795, 0.249808, 0.0, 0.250006, 0.249651, 0.999712],
        [0.249436, 0.249657, 0.16302, 0.249483, 0.249829, 0.0, 0.250601, 0.250526, 0.0, 0.25048, 0.249988, 0.83698],
        [0.249966, 0.249762, 0.549568, 0.250325, 0.250157, 0.0, 0.249972, 0.249798, 0.0, 0.249737, 0.250283, 0.450432],
        [0.250306, 0.250357, 0.649401, 0.249965, 0.2496, 0.0, 0.249576, 0.249904, 0.0, 0.250153, 0.250139, 0.350599],
        [0.249811, 0.249831, 0.346895, 0.249966, 0.24988, 0.0, 0.249287, 0.250303, 0.0, 0.250936, 0.249986, 0.653105],
        [0.249791, 0.249796, 0.235294, 0.249283, 0.24998, 0.0, 0.250749, 0.250415, 0.0, 0.250177, 0.249809, 0.764706],
        [0.249313, 0.249881, 0.000876, 0.250295, 0.250234, 0.0, 0.249915, 0.250297, 0.0, 0.250477, 0.249588, 0.999124],
        [0.25, 0.249584, 0.225182, 0.24997, 0.25012, 0.0, 0.25039, 0.249735, 0.0, 0.24964, 0.250561, 0.774818],
        [0.25012, 0.250009, 0.00571, 0.249533, 0.250692, 0.0, 0.250478, 0.249849, 0.0, 0.249869, 0.24945, 0.99429],
        [0.250163, 0.250189, 0.006864, 0.249471, 0.249734, 0.0, 0.250122, 0.250323, 0.0, 0.250244, 0.249754, 0.993136],
        [0.250434, 0.250787, 0.363928, 0.250982, 0.249748, 0.0, 0.249757, 0.24887, 0.0, 0.248827, 0.250595, 0.636072],
        [0.250371, 0.249711, 0.164744, 0.249794, 0.250359, 0.0, 0.249534, 0.250071, 0.0, 0.250301, 0.249859, 0.835256],
        [0.249999, 0.248938, 0.011846, 0.249957, 0.250115, 0.0, 0.250245, 0.250615, 0.0, 0.249799, 0.250332, 0.988154],
        [0.249479, 0.249719, 0.996934, 0.249831, 0.250216, 0.0, 0.250533, 0.249979, 0.0, 0.250157, 0.250086, 0.003066],
        [0.249843, 0.249837, 0.01015, 0.249509, 0.249165, 0.0, 0.250187, 0.250634, 0.0, 0.250461, 0.250364, 0.98985],
        [0.249824, 0.249789, 0.635469, 0.250801, 0.24921, 0.0, 0.250138, 0.250607, 0.0, 0.249237, 0.250394, 0.364531],
        [0.250264, 0.250461, 0.000703, 0.250092, 0.250474, 0.0, 0.250085, 0.249532, 0.0, 0.249559, 0.249533, 0.999297],
        [0.249993, 0.249784, 0.041916, 0.250077, 0.250111, 0.0, 0.249633, 0.249851, 0.0, 0.250297, 0.250254, 0.958084],
        [0.250289, 0.250251, 0.261772, 0.250604, 0.249861, 0.0, 0.249689, 0.250343, 0.0, 0.249418, 0.249545, 0.738228],
        [0.250033, 0.249955, 0.034293, 0.249915, 0.250166, 0.0, 0.250027, 0.249399, 0.0, 0.250025, 0.25048, 0.965707],
        [0.249957, 0.250477, 0.072674, 0.249896, 0.249079, 0.0, 0.249742, 0.250021, 0.0, 0.250405, 0.250423, 0.927326],
        [0.250095, 0.249599, 0.143966, 0.250258, 0.250184, 0.0, 0.24951, 0.249725, 0.0, 0.250137, 0.250492, 0.856034],
        [0.249712, 0.249792, 0.874205, 0.249509, 0.249756, 0.0, 0.250821, 0.24995, 0.0, 0.249958, 0.250502, 0.125795],
        [0.251171, 0.250308, 0.054919, 0.249709, 0.249682, 0.0, 0.24943, 0.250138, 0.0, 0.24969, 0.249872, 0.945081],
        [0.249675, 0.250404, 0.120439, 0.250079, 0.24957, 0.0, 0.250397, 0.249671, 0.0, 0.249849, 0.250355, 0.879561],
        [0.250262, 0.250147, 0.292474, 0.249543, 0.249762, 0.0, 0.249999, 0.24999, 0.0, 0.250196, 0.250101, 0.707526],
        [0.249929, 0.250321, 0.188351, 0.250245, 0.250566, 0.0, 0.250265, 0.249328, 0.0, 0.249561, 0.249785, 0.811649],
        [0.250434, 0.250222, 0.080336, 0.250028, 0.249938, 0.0, 0.24993, 0.249498, 0.0, 0.249608, 0.250342, 0.919664],
        [0.250672, 0.249404, 0.094141, 0.249637, 0.250039, 0.0, 0.249779, 0.249953, 0.0, 0.249912, 0.250604, 0.905859],
        [0.249594, 0.250446, 0.32657, 0.249856, 0.249004, 0.0, 0.249919, 0.250084, 0.0, 0.250631, 0.250466, 0.67343],
        [0.249426, 0.250203, 0.086658, 0.250329, 0.248801, 0.0, 0.249258, 0.250635, 0.0, 0.250987, 0.250361, 0.913342],
        [0.250121, 0.250611, 0.728075, 0.250273, 0.249288, 0.0, 0.249303, 0.249976, 0.0, 0.250303, 0.250125, 0.271925]])
    yTrue = np.array([[0.70710678, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.70710678],
                      [-0.09248545842787607, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.9957140352427426],
                      [0.9423045179820709, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.3347569198546565],
                      [0.6074383189617029, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.7943668476572903],
                      [0.989754471067357, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.14277985502925955],
                      [0.9843083270873976, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.176457125745629],
                      [0.9353680460474734, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.35367586634280224],
                      [0.9973536394274106, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0727029430002576],
                      [0.9588020793404198, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.2840749419651215],
                      [-0.2149504677659761, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.9766249517635662],
                      [-0.10490662096888521, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.9944820766996711],
                      [-0.295540856258399, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.9553301012121687],
                      [-0.41373585128048856, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.9103969713071378],
                      [-0.41104798182766356, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.911613710205921],
                      [0.9686546129452765, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.24841143455935444],
                      [-0.32875530551414317, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.9444151359949196],
                      [-0.06856799042978376, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.9976464457353723],
                      [0.8231432391123307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.5678337854545645],
                      [-0.17526160933692875, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.9845218983306718],
                      [0.016953388488596535, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.9998562809817992],
                      [-0.40359830196784424, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.9149362877537828],
                      [0.7414461849814653, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.671012335785588],
                      [0.8060313243011339, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.5918728784505676],
                      [0.5890868686725589, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.808069713055476],
                      [0.48504422368442796, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.8744896231919339],
                      [0.02892892728951191, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.999581470999677],
                      [0.4743320854099435, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.8803459960439726],
                      [-0.07657986587608547, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.9970634504094515],
                      [0.08309614441681555, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.9965415349011097],
                      [0.602948415616069, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.7977801753027409],
                      [-0.40625943445937107, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.9137577752953744],
                      [-0.10861936822304523, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.9940834134250639],
                      [0.9985067414061839, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.05462863137956356],
                      [0.10047456048955859, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.994939627663121],
                      [0.7971387299340172, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.6037961951181722],
                      [-0.025700393938978536, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.9996696903234494],
                      [0.2050020295959057, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.978761548009299],
                      [0.5119574481704267, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.8590108097473657],
                      [0.184923598891851, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.9827529000582425],
                      [-0.2700683093167157, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.9628411646283154],
                      [-0.37940584349222783, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.9252303528981046],
                      [0.9350585107652634, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.3544934152356682],
                      [-0.23413974621721126, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.9722029516728181],
                      [0.3465529422590865, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.9380304143318436],
                      [0.5402200829187365, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.8415237738835865],
                      [0.4340065376853208, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.9009097209190275],
                      [0.2837725555844178, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.9588916188480784],
                      [0.3065385663833029, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.9518582390879796],
                      [0.5713007399691656, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.8207408022699273],
                      [0.29418644187935034, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.9557480512218519],
                      [0.8531939182233416, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.5215938438159541]])

    xQM = np.array([[0.249973, 0.250044, 0.49999, 0.250192, 0.24918, 0.0, 0.249474, 0.250612, 0.0, 0.250361, 0.250164, 0.50001]])
    yTrue = np.array([[0.70710678, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.70710678]])

    xQM = np.array([[[0.249973], [0.250044], [0.49999], [0.250192], [0.24918], [0.0], [0.249474], [0.250612], [0.0], [0.250361], [0.250164], [0.50001]], ])
    yTrue = np.array([[[0.70710678], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.70710678]]])



    modelName = 'my_model_middleLayerof12'

    trainModel(modelName)
    model = load_model(f'{modelName}.h5')
    outputModel(model)
    testModel(model)



